{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63eaf39c",
   "metadata": {},
   "source": [
    "This Notebook takes input of the XML files obtained through BROLoket: https://www.broloket.nl/ondergrondgegevens\n",
    "Some minor preprocessing is done.\n",
    "The data is stored in a csv for the CPT and the Bore data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351e492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import folium\n",
    "from geotexxx.gefxml_reader import Bore, Cpt\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fdef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cptdata():\n",
    "    \"\"\"\n",
    "    Function to load the CPT data from XML files,\n",
    "    storing metadata and measurements in a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initiate Cpt instance\n",
    "    xml = Cpt()\n",
    "    df_cpt = pd.DataFrame()\n",
    "    directory = \"../Data/SoildataComplete/Geotechnisch sondeeronderzoek BRO\"\n",
    "\n",
    "    # iterate over files\n",
    "    for file in tqdm(os.listdir(directory)):\n",
    "        \n",
    "        # only extract from the xml files\n",
    "        if file.split(\".\")[1] == 'xml':\n",
    "            xml.load_xml(directory + '/' + file)\n",
    "            \n",
    "            # add metadata\n",
    "            xml.data[['easting', 'northing', 'groundlevel']] \\\n",
    "                = xml.easting, xml.northing, xml.groundlevel\n",
    "            \n",
    "            # append data to dataframe\n",
    "            df_cpt = pd.concat([df_cpt, xml.data])\n",
    "            \n",
    "    return df_cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe9342f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def load_boredata():\n",
    "    \"\"\"\n",
    "    Function to load the Bore data from XML files,\n",
    "    storing metadata and measurements in a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initiate Bore instance\n",
    "    xml = Bore()\n",
    "    directory = \"../Data/soildata/Geotechnisch booronderzoek BRO\"\n",
    "    df_bore = pd.DataFrame()\n",
    "    \n",
    "    # iterate over files\n",
    "    for file in tqdm(os.listdir(directory)):\n",
    "\n",
    "        # skip the IMBRO_A files\n",
    "        if file.split(\".\")[0][-1] == 'A':\n",
    "            continue\n",
    "\n",
    "        xml.load_xml(directory + '/' + file)\n",
    "\n",
    "        # data is stored per 'lab' and 'veld' (research environment)\n",
    "        for key in list(xml.soillayers.keys()):\n",
    "            \n",
    "            # extract the bore data\n",
    "            data = xml.soillayers[key]\n",
    "            \n",
    "            # add the metadata\n",
    "            data[['environment', 'easting', 'northing', 'groundlevel', 'testid', 'finaldepth']] \\\n",
    "                = key, xml.easting, xml.northing, xml.groundlevel, xml.testid, xml.finaldepth\n",
    "            data['date'] = pd.to_datetime(pd.DataFrame(xml.date, index=[0]))[0]\n",
    "            \n",
    "            # append data to dataframe\n",
    "            df_bore = pd.concat([df_bore, data])\n",
    "\n",
    "    return df_bore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5488f64a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10884/10884 [44:58<00:00,  4.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Takes approx 45 minutes to load\n",
    "df_cpt_init = load_cptdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6c8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt = df_cpt_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3667ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:12<00:00, 18.07it/s]\n"
     ]
    }
   ],
   "source": [
    "df_bore_init = load_boredata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca63cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bore = df_bore_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "339168a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, investigation):\n",
    "    \"\"\"\n",
    "    Function to do first preprocessing steps for Bore and CPT data\n",
    "    \"\"\"\n",
    "    \n",
    "    data = data.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    # drop columns with more than 90% nan\n",
    "    to_drop = [col for col in data if data[col].isna().sum()/len(data) >= 0.9]\n",
    "    \n",
    "    # remove noninformative columns from bore data\n",
    "    if investigation == 'bore':\n",
    "        to_drop += ['upperBoundaryDetermination', 'lowerBoundaryDetermination', 'activityType', 'grainshape', 'mixed', 'components', 'layer', 'soil']\n",
    "    \n",
    "    data = data.drop(to_drop, axis=1)\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93e81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bore = preprocess(df_bore, 'bore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371d13b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bore.to_csv('Bore.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9529391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt = preprocess(df_cpt, 'cpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acaed804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt.to_csv('CPT.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
